{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e7PsSmy9sCoR"
      },
      "source": [
        "![logog](https://raw.githubusercontent.com/Pacific-AI-Corp/langtest/main/docs/assets/images/logo.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3o5sAOfwL5qd"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pacific-AI-Corp/langtest/blob/main/demo/tutorials/llm_notebooks/Cohere_QA_Summarization_Testing_Notebook.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WJJzt3RWhEc6"
      },
      "source": [
        "**LangTest** is an open-source python library designed to help developers deliver safe and effective Natural Language Processing (NLP) models. Whether you are using **John Snow Labs, Hugging Face, Spacy** models or **OpenAI, Cohere, AI21, Hugging Face Inference API and Azure-OpenAI** based LLMs, it has got you covered. You can test any Named Entity Recognition (NER), Text Classification, fill-mask, Translation model using the library. We also support testing LLMS for Question-Answering, Summarization and text-generation tasks on benchmark datasets. The library supports 60+ out of the box tests. For a complete list of supported test categories, please refer to the [documentation](http://langtest.org/docs/pages/docs/test_categories).\n",
        "\n",
        "Metrics are calculated by comparing the model's extractions in the original list of sentences against the extractions carried out in the noisy list of sentences. The original annotated labels are not used at any point, we are simply comparing the model against itself in a 2 settings."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "26qXWhCYhHAt"
      },
      "source": [
        "# Getting started with LangTest "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"langtest[evaluate,cohere,transformers]\" "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yR6kjOaiheKN"
      },
      "source": [
        "# Harness and Its Parameters\n",
        "\n",
        "The Harness class is a testing class for Natural Language Processing (NLP) models. It evaluates the performance of a NLP model on a given task using test data and generates a report with test results.Harness can be imported from the LangTest library in the following way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTzSJpMlhgq5"
      },
      "outputs": [],
      "source": [
        "#Import Harness from the LangTest library\n",
        "from langtest import Harness"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sBcZjwJBhkOw"
      },
      "source": [
        "It imports the Harness class from within the module, that is designed to provide a blueprint or framework for conducting NLP testing, and that instances of the Harness class can be customized or configured for different testing scenarios or environments.\n",
        "\n",
        "Here is a list of the different parameters that can be passed to the Harness function:\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "| Parameter  | Description |  \n",
        "| - | - | \n",
        "|**task**     |Task for which the model is to be evaluated (question-answering or summarization)|\n",
        "| **model**     | Specifies the model(s) to be evaluated. This parameter can be provided as either a dictionary or a list of dictionaries. Each dictionary should contain the following keys: <ul><li>model (mandatory): \tPipelineModel or path to a saved model or pretrained pipeline/model from hub.</li><li>hub (mandatory): Hub (library) to use in back-end for loading model from public models hub or from path</li></ul>|\n",
        "| **data**      | The data to be used for evaluation. A dictionary providing flexibility and options for data sources. It should include the following keys: <ul><li>data_source (mandatory): The source of the data.</li><li>subset (optional): The subset of the data.</li><li>feature_column (optional): The column containing the features.</li><li>target_column (optional): The column containing the target labels.</li><li>split (optional): The data split to be used.</li><li>source (optional): Set to 'huggingface' when loading Hugging Face dataset.</li></ul> |\n",
        "| **config**    | Configuration for the tests to be performed, specified in the form of a YAML file. |\n",
        "\n",
        "\n",
        "<br/>\n",
        "<br/>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JFhJ9CcbsKqN"
      },
      "source": [
        "# Cohere Model Testing For Question Answering\n",
        "\n",
        "In this section, we dive into testing of Cohere models in Question Answering task.\n",
        "\n",
        "LangTest supports robustness tests for LLM testing for now."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kKgXC7cvuyar"
      },
      "source": [
        "### Set environment for Cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHqlSjFLuy7o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"COHERE_API_KEY\"] = \"<YOUR_API_KEY>\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BoolQ-test-tiny dataset testing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "swaYPW-wPlku"
      },
      "source": [
        "### Setup and Configure Harness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "harness = Harness(\n",
        "                  task=\"question-answering\", \n",
        "                  model={\"model\": \"command-xlarge-nightly\", \"hub\":\"cohere\"}, \n",
        "                  data={\"data_source\" :\"BoolQ\",\n",
        "                        \"split\":\"test-tiny\"}\n",
        "                  )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jWPAw9q0PwD1"
      },
      "source": [
        "We have specified task as QA, hub as Cohere and model as `command-xlarge-nightly`.\n",
        "\n",
        "For dataset we used `BoolQ` dataset and `test-tiny` split which includes 50 samples. Other available datasets are: [Benchmark Datasets](https://langtest.org/docs/pages/docs/data#question-answering)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For tests we used lowercase and uppercase. Other available robustness tests for QA task are:\n",
        "* `add_context`\n",
        "* `add_contraction`\n",
        "* `add_punctuation`\n",
        "* `add_typo`\n",
        "* `add_ocr_typo`\n",
        "* `american_to_british`\n",
        "* `british_to_american`\n",
        "* `lowercase`\n",
        "* `strip_punctuation`\n",
        "* `titlecase`\n",
        "* `uppercase`\n",
        "* `number_to_word`\n",
        "* `add_abbreviation`\n",
        "* `add_speech_to_text_typo`\n",
        "* `add_slangs`\n",
        "* `dyslexia_word_swap`\n",
        "* `multiple_perturbations`\n",
        "* `adjective_synonym_swap`\n",
        "* `adjective_antonym_swap`\n",
        "* `strip_all_punctuation`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available Bias tests for QA task are:\n",
        "\n",
        "* `replace_to_male_pronouns`\n",
        "* `replace_to_female_pronouns`\n",
        "* `replace_to_neutral_pronouns`\n",
        "* `replace_to_high_income_country`\n",
        "* `replace_to_low_income_country`\n",
        "* `replace_to_upper_middle_income_country`\n",
        "* `replace_to_lower_middle_income_country`\n",
        "* `replace_to_white_firstnames`\n",
        "* `replace_to_black_firstnames`\n",
        "* `replace_to_hispanic_firstnames`\n",
        "* `replace_to_asian_firstnames`\n",
        "* `replace_to_white_lastnames`\n",
        "* `replace_to_sikh_names`\n",
        "* `replace_to_christian_names`\n",
        "* `replace_to_hindu_names`\n",
        "* `replace_to_muslim_names`\n",
        "* `replace_to_inter_racial_lastnames`\n",
        "* `replace_to_native_american_lastnames`\n",
        "* `replace_to_asian_lastnames`\n",
        "* `replace_to_hispanic_lastnames`\n",
        "* `replace_to_black_lastnames`\n",
        "* `replace_to_parsi_names`\n",
        "* `replace_to_jain_names`\n",
        "* `replace_to_buddhist_names`\n",
        "\n",
        "Available Representation tests for QA task are:\n",
        "\n",
        "* `min_gender_representation_count`\n",
        "* `min_ethnicity_name_representation_count`\n",
        "* `min_religion_name_representation_count`\n",
        "* `min_country_economic_representation_count`\n",
        "* `min_gender_representation_proportion`\n",
        "* `min_ethnicity_name_representation_proportion`\n",
        "* `min_religion_name_representation_proportion`\n",
        "* `min_country_economic_representation_proportion`\n",
        "\n",
        "\n",
        "Available Accuracy tests for QA task are:\n",
        "\n",
        "* `min_exact_match_score`\n",
        "* `min_bleu_score`\n",
        "* `min_rouge1_score`\n",
        "* `min_rouge2_score`\n",
        "* `min_rougeL_score`\n",
        "* `min_rougeLsum_score`\n",
        "\n",
        "\n",
        "Available Fairness tests for QA task are:\n",
        "\n",
        "* `max_gender_rouge1_score`\n",
        "* `max_gender_rouge2_score`\n",
        "* `max_gender_rougeL_score`\n",
        "* `max_gender_rougeLsum_score`\n",
        "* `min_gender_rouge1_score`\n",
        "* `min_gender_rouge2_score`\n",
        "* `min_gender_rougeL_score`\n",
        "* `min_gender_rougeLsum_score`\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also set prompts and other model parameters in config. Possible parameters are:\n",
        "* `user_promt:` Promt to be given to the model.\n",
        "* `temperature:` Temperature of the model.\n",
        "* `max_tokens:` Maximum number of output tokens allowed for model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-ghKYLpPqtt",
        "outputId": "20f1a53b-2745-4028-bbe7-8ce4dccb745e"
      },
      "outputs": [],
      "source": [
        "harness.configure({\n",
        "    'model_parameters': {\n",
        "      'max_tokens': 64\n",
        "    },\n",
        "    \n",
        "    'tests': {\n",
        "      'defaults':{\n",
        "        'min_pass_rate': 1.00\n",
        "      },\n",
        "\n",
        "      'robustness':{\n",
        "        'lowercase': {'min_pass_rate': 0.70},\n",
        "        'uppercase': {'min_pass_rate': 0.70}\n",
        "      }\n",
        "    }\n",
        "})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPU46A7WigFr"
      },
      "source": [
        "Here we have configured the harness to perform two robustness tests (uppercase and lowercase) and defined the minimum pass rate for each test."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "➤ You can adjust the level of transformation in the sentence by using the \"`prob`\" parameter, which controls the proportion of words to be changed during robustness tests.\n",
        "\n",
        "➤ **NOTE** : \"`prob`\" defaults to 1.0, which means all words will be transformed.\n",
        "```\n",
        "harness.configure(\n",
        "{\n",
        " 'tests': {\n",
        "    'defaults': {'min_pass_rate': 0.65},\n",
        "      'robustness': {\n",
        "        'lowercase': {'min_pass_rate': 0.66, 'prob': 0.50}, \n",
        "        'uppercase':{'min_pass_rate': 0.60, 'prob': 0.70},\n",
        "      }\n",
        "  }\n",
        "})\n",
        "\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i6kPvA13F7cr"
      },
      "source": [
        "\n",
        "### Generating the test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdNH3wCKF9fn",
        "outputId": "cd348490-7ade-40fa-d870-dc059f5aa647"
      },
      "outputs": [],
      "source": [
        "harness.generate()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nyjDdYLeGCmM"
      },
      "source": [
        "harness.generate() method automatically generates the test cases (based on the provided configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "c0jL1_G7F_p6",
        "outputId": "502c1525-9000-4041-823b-3b04f6650892"
      },
      "outputs": [],
      "source": [
        "harness.testcases()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NOJ8BAU2GGzd"
      },
      "source": [
        "harness.testcases() method displays the produced test cases in form of a pandas data frame."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3CwhQw6hGR9S"
      },
      "source": [
        "### Running the tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aguX6-aFGOnP",
        "outputId": "bb014811-522b-4f07-fa8a-bf3d1c906d7f"
      },
      "outputs": [],
      "source": [
        "harness.run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "191O2oaUGWrH"
      },
      "source": [
        "Called after harness.generate() and is to used to run all the tests.  Returns a pass/fail flag for each test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "-cXkdnihGYke",
        "outputId": "2aa88caa-5e83-44fe-b3aa-b81b9ae9115a"
      },
      "outputs": [],
      "source": [
        "harness.generated_results()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TKB8Rsr2GZME"
      },
      "source": [
        "This method returns the generated results in the form of a pandas dataframe, which provides a convenient and easy-to-use format for working with the test results. You can use this method to quickly identify the test cases that failed and to determine where fixes are needed."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PBSlpWnUU55G"
      },
      "source": [
        "### Final Results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can call `.report()` which summarizes the results giving information about pass and fail counts and overall test pass/fail flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "gp57HcF9yxi7",
        "outputId": "b893072f-102a-45a6-be03-d737996e659c"
      },
      "outputs": [],
      "source": [
        "harness.report()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5N0cKfKiLsiQ"
      },
      "source": [
        "#### NQ-open-test dataset testing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use another dataset, NQ-open for testing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "harness = Harness(\n",
        "                  task=\"question-answering\", \n",
        "                  model={\"model\": \"command-xlarge-nightly\",\"hub\":\"cohere\"}, \n",
        "                  data={\"data_source\" :\"NQ-open\",\n",
        "                        \"split\":\"test-tiny\"}\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnLWJkPVEDmg",
        "outputId": "92ca0633-a1c6-4de3-f9fd-c77e6bcb5374"
      },
      "outputs": [],
      "source": [
        "harness.configure({\n",
        "    'model_parameters': {\n",
        "      'max_tokens': 64\n",
        "    },\n",
        "    'tests': {\n",
        "      'defaults':{\n",
        "        'min_pass_rate': 1.00\n",
        "      },\n",
        "\n",
        "      'robustness':{\n",
        "        'lowercase': {'min_pass_rate': 0.70},\n",
        "        'uppercase': {'min_pass_rate': 0.70}\n",
        "      }\n",
        "    }\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3U0kM62EG6B",
        "outputId": "1ad54c30-3371-41b6-e85c-4dc69ffcd8aa"
      },
      "outputs": [],
      "source": [
        "harness.generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "0ipwMsy0EHYY",
        "outputId": "a3e9c1fb-22dc-416d-8cf6-043ac4b28432"
      },
      "outputs": [],
      "source": [
        "harness.testcases()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nic5HRZEJu5",
        "outputId": "dbbf911a-413e-479c-996b-98430920f0b5"
      },
      "outputs": [],
      "source": [
        "harness.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjZc-ZcCELbU"
      },
      "outputs": [],
      "source": [
        "harness.generated_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "PlrAxK1eENmh",
        "outputId": "7fd59473-20ac-402b-a39b-e5e3e29cf1f4"
      },
      "outputs": [],
      "source": [
        "harness.report()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cohere Model Testing For Summarization\n",
        "\n",
        "In this section, we dive into testing of Cohere models in Summarization task.\n",
        "\n",
        "LangTest supports robustness tests for LLM testing for now."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XSum-test-tiny dataset testing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup and configure harness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "harness = Harness(\n",
        "                  task=\"summarization\", \n",
        "                  model={\"model\": \"command-xlarge-nightly\", \"hub\":\"cohere\"},\n",
        "                  data={\"data_source\" :\"XSum\",\n",
        "                        \"split\":\"test-tiny\"}\n",
        "                  )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have specified task as summarization, hub as Cohere and model as `command-xlarge-nightly`.\n",
        "\n",
        "\n",
        "For dataset we used `XSum` dataset and `test-tiny` split which includes 50 samples. Other available datasets are: [Benchmark Datasets](https://langtest.org/docs/pages/docs/data#summarization)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For tests we used lowercase and uppercase. Other available robustness tests for summarization task are:\n",
        "\n",
        "* `add_context`\n",
        "* `add_contraction`\n",
        "* `add_punctuation`\n",
        "* `add_typo`\n",
        "* `add_ocr_typo`\n",
        "* `american_to_british`\n",
        "* `british_to_american`\n",
        "* `lowercase`\n",
        "* `strip_punctuation`\n",
        "* `titlecase`\n",
        "* `uppercase`\n",
        "* `number_to_word`\n",
        "* `add_abbreviation`\n",
        "* `add_speech_to_text_typo`\n",
        "* `add_slangs`\n",
        "* `dyslexia_word_swap`\n",
        "* `multiple_perturbations`\n",
        "* `adjective_synonym_swap`\n",
        "* `adjective_antonym_swap`\n",
        "\n",
        "Available Bias tests for summarization task are:\n",
        "\n",
        "* `replace_to_male_pronouns`\n",
        "* `replace_to_female_pronouns`\n",
        "* `replace_to_neutral_pronouns`\n",
        "* `replace_to_high_income_country`\n",
        "* `replace_to_low_income_country`\n",
        "* `replace_to_upper_middle_income_country`\n",
        "* `replace_to_lower_middle_income_country`\n",
        "* `replace_to_white_firstnames`\n",
        "* `replace_to_black_firstnames`\n",
        "* `replace_to_hispanic_firstnames`\n",
        "* `replace_to_asian_firstnames`\n",
        "* `replace_to_white_lastnames`\n",
        "* `replace_to_sikh_names`\n",
        "* `replace_to_christian_names`\n",
        "* `replace_to_hindu_names`\n",
        "* `replace_to_muslim_names`\n",
        "* `replace_to_inter_racial_lastnames`\n",
        "* `replace_to_native_american_lastnames`\n",
        "* `replace_to_asian_lastnames`\n",
        "* `replace_to_hispanic_lastnames`\n",
        "* `replace_to_black_lastnames`\n",
        "* `replace_to_parsi_names`\n",
        "* `replace_to_jain_names`\n",
        "* `replace_to_buddhist_names`\n",
        "\n",
        "Available Representation tests for summarization task are:\n",
        "\n",
        "* `min_gender_representation_count`\n",
        "* `min_ethnicity_name_representation_count`\n",
        "* `min_religion_name_representation_count`\n",
        "* `min_country_economic_representation_count`\n",
        "* `min_gender_representation_proportion`\n",
        "* `min_ethnicity_name_representation_proportion`\n",
        "* `min_religion_name_representation_proportion`\n",
        "* `min_country_economic_representation_proportion`\n",
        "\n",
        "\n",
        "Available Accuracy tests for summarization task are:\n",
        "\n",
        "* `min_exact_match_score`\n",
        "* `min_bleu_score`\n",
        "* `min_rouge1_score`\n",
        "* `min_rouge2_score`\n",
        "* `min_rougeL_score`\n",
        "* `min_rougeLsum_score`\n",
        "\n",
        "\n",
        "Available Fairness tests for summarization task are:\n",
        "\n",
        "* `max_gender_rouge1_score`\n",
        "* `max_gender_rouge2_score`\n",
        "* `max_gender_rougeL_score`\n",
        "* `max_gender_rougeLsum_score`\n",
        "* `min_gender_rouge1_score`\n",
        "* `min_gender_rouge2_score`\n",
        "* `min_gender_rougeL_score`\n",
        "* `min_gender_rougeLsum_score`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tests': {'defaults': {'min_pass_rate': 0.65},\n",
              "  'robustness': {'uppercase': {'min_pass_rate': 0.66},\n",
              "   'lowercase': {'min_pass_rate': 0.6}}}}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "harness.configure(\n",
        "{\n",
        " 'tests': {'defaults': {'min_pass_rate': 0.65},\n",
        "           'robustness': {'uppercase': {'min_pass_rate': 0.66}, \n",
        "                          'lowercase':{'min_pass_rate': 0.60},\n",
        "                        }\n",
        "          }\n",
        " }\n",
        " )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Here we have configured the harness to perform two robustness tests (uppercase and lowercase) and defined the minimum pass rate for each test.`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating the Test Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating testcases...: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "harness.generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>test_type</th>\n",
              "      <th>original</th>\n",
              "      <th>test_case</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>robustness</td>\n",
              "      <td>uppercase</td>\n",
              "      <td>The ex-Reading defender denied fraudulent trad...</td>\n",
              "      <td>THE EX-READING DEFENDER DENIED FRAUDULENT TRAD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>robustness</td>\n",
              "      <td>lowercase</td>\n",
              "      <td>The ex-Reading defender denied fraudulent trad...</td>\n",
              "      <td>the ex-reading defender denied fraudulent trad...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     category  test_type                                           original   \n",
              "0  robustness  uppercase  The ex-Reading defender denied fraudulent trad...  \\\n",
              "1  robustness  lowercase  The ex-Reading defender denied fraudulent trad...   \n",
              "\n",
              "                                           test_case  \n",
              "0  THE EX-READING DEFENDER DENIED FRAUDULENT TRAD...  \n",
              "1  the ex-reading defender denied fraudulent trad...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "harness.testcases()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running testcases... :   0%|          | 0/2 [00:00<?, ?it/s]Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n",
            "Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n",
            "Running testcases... :  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n",
            "Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n",
            "Running testcases... : 100%|██████████| 2/2 [00:05<00:00,  2.77s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "harness.run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generated Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>test_type</th>\n",
              "      <th>original</th>\n",
              "      <th>test_case</th>\n",
              "      <th>expected_result</th>\n",
              "      <th>actual_result</th>\n",
              "      <th>eval_score</th>\n",
              "      <th>pass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>robustness</td>\n",
              "      <td>uppercase</td>\n",
              "      <td>The ex-Reading defender denied fraudulent trad...</td>\n",
              "      <td>THE EX-READING DEFENDER DENIED FRAUDULENT TRAD...</td>\n",
              "      <td>\\nSam Sodje, the ex-Reading defender, has deni...</td>\n",
              "      <td>\\nFormer Reading defender Sam Sodje has denied...</td>\n",
              "      <td>0.851064</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>robustness</td>\n",
              "      <td>lowercase</td>\n",
              "      <td>The ex-Reading defender denied fraudulent trad...</td>\n",
              "      <td>the ex-reading defender denied fraudulent trad...</td>\n",
              "      <td>\\nSam Sodje, 37, has denied fraudulent trading...</td>\n",
              "      <td>\\nFour brothers have denied fraud charges rela...</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     category  test_type                                           original   \n",
              "0  robustness  uppercase  The ex-Reading defender denied fraudulent trad...  \\\n",
              "1  robustness  lowercase  The ex-Reading defender denied fraudulent trad...   \n",
              "\n",
              "                                           test_case   \n",
              "0  THE EX-READING DEFENDER DENIED FRAUDULENT TRAD...  \\\n",
              "1  the ex-reading defender denied fraudulent trad...   \n",
              "\n",
              "                                     expected_result   \n",
              "0  \\nSam Sodje, the ex-Reading defender, has deni...  \\\n",
              "1  \\nSam Sodje, 37, has denied fraudulent trading...   \n",
              "\n",
              "                                       actual_result  eval_score   pass  \n",
              "0  \\nFormer Reading defender Sam Sodje has denied...    0.851064   True  \n",
              "1  \\nFour brothers have denied fraud charges rela...    0.307692  False  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "harness.generated_results()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Report"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can call `.report()` which summarizes the results giving information about pass and fail counts and overall test pass/fail flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>test_type</th>\n",
              "      <th>fail_count</th>\n",
              "      <th>pass_count</th>\n",
              "      <th>pass_rate</th>\n",
              "      <th>minimum_pass_rate</th>\n",
              "      <th>pass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>robustness</td>\n",
              "      <td>uppercase</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>100%</td>\n",
              "      <td>66%</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>robustness</td>\n",
              "      <td>lowercase</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0%</td>\n",
              "      <td>60%</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     category  test_type  fail_count  pass_count pass_rate minimum_pass_rate   \n",
              "0  robustness  uppercase           0           1      100%               66%  \\\n",
              "1  robustness  lowercase           1           0        0%               60%   \n",
              "\n",
              "    pass  \n",
              "0   True  \n",
              "1  False  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "harness.report()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
