{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logog](https://raw.githubusercontent.com/Pacific-AI-Corp/langtest/main/docs/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pacific-AI-Corp/langtest/blob/main/demo/tutorials/misc/Custom_Hub_Notebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we will delve into the powerful capabilities of the **LangTest** open-source Python library. This library serves as an indispensable tool for data scientists and developers engaged in Natural Language Processing (NLP) model evaluation. Regardless of your preference for established models like those from **John Snow Labs, Hugging Face, Spacy**, or cutting-edge options such as **OpenAI, Cohere, AI21, Hugging Face Inference API, and Azure-OpenAI**, LangTest has the versatility to support them all.\n",
    "\n",
    "The key focus of LangTest lies in facilitating the assessment of various NLP model aspects. You can test any Named Entity Recognition (NER), Text Classification, fill-mask, Translation model using the library. We also support testing LLMS for Question-Answering, Summarization and text-generation tasks on benchmark datasets. The library supports 60+ out of the box tests. For a complete list of supported test categories, please refer to the [documentation](http://langtest.org/docs/pages/docs/test_categories).\n",
    "\n",
    "Getting started with LangTest is a breeze. Simply execute the following command in your terminal or command prompt:\n",
    "\n",
    "```bash\n",
    "pip install langtest\n",
    "```\n",
    "\n",
    "Once you've installed the library, we'll demonstrate how to leverage its functionalities within this Notebook. Specifically, we'll showcase how LangTest can be used to evaluate the robustness, bias, accuracy, performance, security, fairness, toxicity, translation, representation, and clinical aspects of your customized NLP models. Follow along to harness the full potential of LangTest for your NLP evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# text processing libraries\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "# nltk libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# import langtest \n",
    "from langtest import Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download imdb dataset\n",
    "!wget https://raw.githubusercontent.com/JohnSnowLabs/langtest/main/demo/data/imdb.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Pre-Processing phase, we'll be using the **IMDB Movie Reviews Dataset**. This dataset contains around 25,000 highly polarized reviews, with around 12,500 reviews each for training and testing. The dataset having the sentence and label columns, where the sentence column contains the review text, and the label column contains the sentiment label (0 for negative and 1 for positive). We'll be using this dataset to train a **Text Classification** model. The model will be trained to classify the sentiment of the movie review as either positive or negative. \n",
    "\n",
    "`review_to_words` function is used to clean the text data. It removes the HTML tags, punctuations, and stopwords from the text data. It also converts the text data to lowercase and lemmatizes the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "negative    10974\n",
       "positive    10847\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('./imdb.csv')\n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Sentence']\n",
    "y = df['class'].apply(lambda x: 1 if x == 'positive' else 0) # Convert the target to numerical values (0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to store cache files\n",
    "cache_dir = os.path.join(\"./\", \"cache/sentiment_analysis\")\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "\n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "\n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                                                              cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "\n",
    "    return words_train, words_test, labels_train, labels_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(data, vocab_size=5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    words = [word for sens in data for word in sens] \n",
    "    \n",
    "    counter = Counter(words)\n",
    "    word_count = dict(counter.most_common(vocab_size - 2))\n",
    "    \n",
    "    word_dict = {word: idx + 2 for idx, word in enumerate(word_count)}\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(X_train + X_test)\n",
    "\n",
    "with open(\"./sentiment_analysis/word_dict.pkl\", \"wb\") as f:\n",
    "  pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_len = convert_and_pad_data(word_dict, X_train)\n",
    "X_test, X_test_len = convert_and_pad_data(word_dict, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array length of the train_X :- 500\n",
      "Length of the train_X_len :- 296\n"
     ]
    }
   ],
   "source": [
    "# print(X_train[2])\n",
    "print(\"Array length of the train_X :- {}\".format(len(X_train[2])))\n",
    "print(\"Length of the train_X_len :- {}\".format(X_train_len[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the transformed dataset to a csv file in locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./sentiment_analysis/\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "pd.concat([pd.DataFrame(y_train.values), pd.DataFrame(X_train_len), pd.DataFrame(X_train)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([pd.DataFrame(y_test.values), pd.DataFrame(X_test_len), pd.DataFrame(X_test)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in only the first 250 rows\n",
    "train_data = pd.read_csv(os.path.join(\n",
    "    './sentiment_analysis/train.csv'), header=None, names=None)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_y = torch.from_numpy(train_data[0].values).float().squeeze()\n",
    "train_X = torch.from_numpy(train_data.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_ds = torch.utils.data.TensorDataset(\n",
    "    train_X, train_y)\n",
    "# Build the dataloader\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of NLP Model with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple RNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the model by settingg up the various layers.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        self.word_dict = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        x = x.t()\n",
    "        lengths = x[0, :]\n",
    "        reviews = x[1:, :]\n",
    "        embeds = self.embedding(reviews)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dense(lstm_out)\n",
    "        out = out[lengths - 1, range(len(lengths))]\n",
    "        return self.sig(out.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis:\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model = LSTMClassifier(embedding_dim, hidden_dim, vocab_size)\n",
    "        self.word_dict = pickle.load(open(\"./sentiment_analysis/word_dict.pkl\", \"rb\"))\n",
    "\n",
    "    def train(self, train_loader, epochs):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                batch_X, batch_y = batch\n",
    "\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "\n",
    "                # TODO: Complete this train method to train the model provided.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                model_output = self.model.forward(batch_X)\n",
    "                loss = loss_fn(model_output, batch_y)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.data.item()\n",
    "            print(\"Epoch: {}, BCELoss: {}\".format(\n",
    "                epoch, total_loss / len(train_loader)))\n",
    "\n",
    "    def predict(self, x):\n",
    "        data_X, data_len = convert_and_pad(self.word_dict, review_to_words(x), pad=500)\n",
    "        data_pack = np.hstack((data_len, data_X))\n",
    "        data_pack = data_pack.reshape(1, -1)\n",
    "        \n",
    "        data = torch.from_numpy(data_pack)\n",
    "        data = data.to(self.device)\n",
    "\n",
    "        # Make sure to put the model into evaluation mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(data)\n",
    "            return \"positive\" if round(output.item()) else \"negative\"\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(x)\n",
    "            predicted = output.data.round()\n",
    "            correct = (predicted == y).sum().item()\n",
    "            return correct / len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentAnalysis(32, 100, 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BCELoss: 0.6145765907423837\n",
      "Epoch: 2, BCELoss: 0.5105757576227188\n",
      "Epoch: 3, BCELoss: 0.39539344251155856\n",
      "Epoch: 4, BCELoss: 0.3329727892790522\n",
      "Epoch: 5, BCELoss: 0.29848648594958443\n"
     ]
    }
   ],
   "source": [
    "model.train(train_loader=train_dl, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(\n",
    "    './sentiment_analysis/test.csv'), header=None, names=None)\n",
    "\n",
    "test_y = torch.from_numpy(test_data[0].values).float().squeeze()\n",
    "test_X = torch.from_numpy(test_data.drop([0], axis=1).values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8561282932416953"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"I am happy with the product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"I am not feeling good today\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's testing the model with Langtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangTest**is an open-source python library designed to help developers deliver safe and effective Natural Language Processing (NLP) models. Whether you are using **John Snow Labs, Hugging Face, Spacy**\n",
    "models or **OpenAI, Cohere, AI21, Hugging Face Inference API and Azure-OpenAI** based LLMs, it has got you covered. You can test any Named Entity Recognition (NER), Text Classification, fill-mask, Translation model using the library. We also support testing LLMS for Question-Answering, Summarization and text-generation tasks on benchmark datasets. The library supports 60+ out of the box tests. For a complete list of supported test categories, please refer to the [documentation](http://langtest.org/docs/pages/docs/test_categories).\n",
    "\n",
    "Metrics are calculated by comparing the model's extractions in the original list of sentences against the extractions carried out in the noisy list of sentences. The original annotated labels are not used at any point, we are simply comparing the model against itself in a 2 settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langtest import Harness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It imports the Harness class from within the module, that is designed to provide a blueprint or framework for conducting NLP testing, and that instances of the Harness class can be customized or configured for different testing scenarios or environments.\n",
    "\n",
    "Here is a list of the different parameters that can be passed to the Harness function:\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "| Parameter     | Description |\n",
    "| - | - |\n",
    "| **task**      | Task for which the model is to be evaluated (text-classification or ner) |\n",
    "| **model**     | Specifies the model(s) to be evaluated. This parameter can be provided as either a dictionary or a list of dictionaries. Each dictionary should contain the following keys: <ul><li>model (mandatory): \tPipelineModel or path to a saved model or pretrained pipeline/model from hub.</li><li>hub (mandatory): Hub (library) to use in back-end for loading model from public models hub, from path or use `custom`</li></ul>|\n",
    "| **data**      | The data to be used for evaluation. A dictionary providing flexibility and options for data sources. It should include the following keys: <ul><li>data_source (mandatory): The source of the data.</li><li>subset (optional): The subset of the data.</li><li>feature_column (optional): The column containing the features.</li><li>target_column (optional): The column containing the target labels.</li><li>split (optional): The data split to be used.</li><li>source (optional): Set to 'huggingface' when loading Hugging Face dataset.</li></ul> |\n",
    "| **config**    | Configuration for the tests to be performed, specified in the form of a YAML file. |\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Configuration : \n",
      " {\n",
      " \"tests\": {\n",
      "  \"defaults\": {\n",
      "   \"min_pass_rate\": 1.0\n",
      "  },\n",
      "  \"robustness\": {\n",
      "   \"add_typo\": {\n",
      "    \"min_pass_rate\": 0.7\n",
      "   },\n",
      "   \"american_to_british\": {\n",
      "    \"min_pass_rate\": 0.7\n",
      "   }\n",
      "  },\n",
      "  \"accuracy\": {\n",
      "   \"min_micro_f1_score\": {\n",
      "    \"min_score\": 0.7\n",
      "   }\n",
      "  },\n",
      "  \"bias\": {\n",
      "   \"replace_to_female_pronouns\": {\n",
      "    \"min_pass_rate\": 0.7\n",
      "   },\n",
      "   \"replace_to_low_income_country\": {\n",
      "    \"min_pass_rate\": 0.7\n",
      "   }\n",
      "  },\n",
      "  \"fairness\": {\n",
      "   \"min_gender_f1_score\": {\n",
      "    \"min_score\": 0.6\n",
      "   }\n",
      "  },\n",
      "  \"representation\": {\n",
      "   \"min_label_representation_count\": {\n",
      "    \"min_count\": 50\n",
      "   }\n",
      "  }\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "harness = Harness(task=\"text-classification\",\n",
    "                  model={'model': model, \"hub\": \"custom\"}, data={'data_source': './data/imdb.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tests': {'defaults': {'min_pass_rate': 0.65},\n",
       "  'robustness': {'add_contraction': {'min_pass_rate': 0.7},\n",
       "   'lowercase': {'min_pass_rate': 0.7}}}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harness.configure(\n",
    "    {\n",
    "        'tests': {\n",
    "            'defaults': {'min_pass_rate': 0.65},\n",
    "            'robustness': {\n",
    "                'add_contraction': {'min_pass_rate': 0.7},\n",
    "                'lowercase': {'min_pass_rate': 0.7},\n",
    "            }\n",
    "        }}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.data = harness.data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating testcases...: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "WARNING:root:Removing samples where no transformation has been applied:\n",
      "- Test 'add_contraction': 19 samples removed out of 100\n",
      "\n",
      "Running testcases... : 100%|██████████| 181/181 [00:53<00:00,  3.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harness.generate().run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>test_type</th>\n",
       "      <th>fail_count</th>\n",
       "      <th>pass_count</th>\n",
       "      <th>pass_rate</th>\n",
       "      <th>minimum_pass_rate</th>\n",
       "      <th>pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robustness</td>\n",
       "      <td>add_contraction</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>98%</td>\n",
       "      <td>70%</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robustness</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100%</td>\n",
       "      <td>70%</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category        test_type  fail_count  pass_count pass_rate  \\\n",
       "0  robustness  add_contraction           2          79       98%   \n",
       "1  robustness        lowercase           0         100      100%   \n",
       "\n",
       "  minimum_pass_rate  pass  \n",
       "0               70%  True  \n",
       "1               70%  True  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harness.report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = harness.generated_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>test_type</th>\n",
       "      <th>original</th>\n",
       "      <th>test_case</th>\n",
       "      <th>expected_result</th>\n",
       "      <th>actual_result</th>\n",
       "      <th>pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robustness</td>\n",
       "      <td>add_contraction</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robustness</td>\n",
       "      <td>add_contraction</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robustness</td>\n",
       "      <td>add_contraction</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robustness</td>\n",
       "      <td>add_contraction</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robustness</td>\n",
       "      <td>add_contraction</td>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category        test_type  \\\n",
       "0  robustness  add_contraction   \n",
       "1  robustness  add_contraction   \n",
       "2  robustness  add_contraction   \n",
       "3  robustness  add_contraction   \n",
       "4  robustness  add_contraction   \n",
       "\n",
       "                                            original  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "4  I sure would like to see a resurrection of a u...   \n",
       "\n",
       "                                           test_case expected_result  \\\n",
       "0  One of the other reviewers has mentioned that ...        negative   \n",
       "1  A wonderful little production. <br /><br />The...        positive   \n",
       "2  I thought this was a wonderful way to spend ti...        negative   \n",
       "3  Petter Mattei's \"Love in the Time of Money\" is...        positive   \n",
       "4  I sure would like to see a resurrection of a u...        positive   \n",
       "\n",
       "  actual_result   pass  \n",
       "0      positive  False  \n",
       "1      positive   True  \n",
       "2      negative   True  \n",
       "3      positive   True  \n",
       "4      positive   True  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "original: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
      "\n",
      "testcase: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They're right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this isn't a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It's called OZ as that's the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy isn't high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I'd say the main appeal of the show's due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what's uncomfortable viewing....thats if you can get in touch with your darker side.\n",
      "\n",
      "diff => (original, testcase): \n",
      " {\"They're\", \"show's\", \"what's\", \"that's\", \"/>It's\", \"isn't\", \"/>I'd\"} \n",
      "\n",
      "expected: negative\n",
      "\n",
      "actual: positive\n",
      "\n",
      "====================================================================================================\n",
      "original: What happened? What we have here is basically a solid and plausible premise and with a decent and talented cast, but somewhere the movie loses it. Actually, it never really got going. There was a little excitement when we find out that Angie is not really pregnant, then find out that she is after all, but that was it. Steve Martin, who is a very talented person and usually brings a lot to a movie, was dreadful and his entire character was not even close to being important to this movie, other than to make it longer. I really would have liked to see more interactions between the main characters, Kate and Angie, and maybe try not for a pure comedy, which unfortunately it was not, but maybe a drama with comedic elements. I think if the movie did this it could have been very funny since both actresses are quite funny in their own ways and sitting here I can think of numerous scenarios that would have been a riot.\n",
      "\n",
      "testcase: What happened? What we have here is basically a solid and plausible premise and with a decent and talented cast, but somewhere the movie loses it. Actually, it never really got going. There was a little excitement when we find out that Angie isn't really pregnant, then find out that she's after all, but that was it. Steve Martin, who's a very talented person and usually brings a lot to a movie, was dreadful and his entire character wasn't even close to being important to this movie, other than to make it longer. I really would have liked to see more interactions between the main characters, Kate and Angie, and maybe try not for a pure comedy, which unfortunately it wasn't, but maybe a drama with comedic elements. I think if the movie did this it could have been very funny since both actresses are quite funny in their own ways and sitting here I can think of numerous scenarios that'd have been a riot.\n",
      "\n",
      "diff => (original, testcase): \n",
      " {\"who's\", \"isn't\", \"that'd\", \"wasn't,\", \"she's\", \"wasn't\"} \n",
      "\n",
      "expected: negative\n",
      "\n",
      "actual: positive\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"{:=<100}\".format(\"=\"))\n",
    "for idx, rows in t_df[t_df['pass'] == False].iterrows():\n",
    "    print(\"original: {}\\n\".format(rows['original']))\n",
    "    print(\"testcase: {}\\n\".format(rows['test_case']))\n",
    "\n",
    "    print(\"diff => (original, testcase): \\n\", set(rows['test_case'].split()) - set(rows['original'].split()), \"\\n\")\n",
    "\n",
    "    print(\"expected: {}\\n\".format(rows['expected_result']))\n",
    "    print(\"actual: {}\\n\".format(rows['actual_result']))\n",
    "    print(\"{:=<100}\".format(\"=\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
