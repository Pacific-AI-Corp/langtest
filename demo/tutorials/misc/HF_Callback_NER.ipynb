{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![logog](https://raw.githubusercontent.com/Pacific-AI-Corp/langtest/main/docs/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pacific-AI-Corp/langtest/blob/main/demo/tutorials/misc/HF_Callback_NER.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**LangTest** is an open-source python library designed to help developers deliver safe and effective Natural Language Processing (NLP) models. Whether you are using **John Snow Labs, Hugging Face, Spacy** models or **OpenAI, Cohere, AI21, Hugging Face Inference API and Azure-OpenAI** based LLMs, it has got you covered. You can test any Named Entity Recognition (NER), Text Classification, fill-mask, Translation model using the library. We also support testing LLMS for Question-Answering, Summarization and text-generation tasks on benchmark datasets. The library supports 60+ out of the box tests. For a complete list of supported test categories, please refer to the [documentation](http://langtest.org/docs/pages/docs/test_categories).\n",
        "\n",
        "Metrics are calculated by comparing the model's extractions in the original list of sentences against the extractions carried out in the noisy list of sentences. The original annotated labels are not used at any point, we are simply comparing the model against itself in a 2 settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting started with LangTest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"langtest[transformers, evaluate]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangTestCallback and Its Parameters\n",
        "\n",
        "The LangTestCallback class is a testing class for Natural Language Processing (NLP) models. It evaluates the performance of a NLP model on a given task using test data and generates a report with test results. It can be imported from the LangTest library in the following way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Import Harness from the LangTest library\n",
        "from langtest.callback import LangTestCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It imports the callback class from within the module, that is designed to provide a blueprint or framework for conducting NLP testing, and instances of the callback class can be customized or configured for different testing scenarios or environments then provided to the trainer.\n",
        "\n",
        "Here is a list of the different parameters that can be passed to the LangTestCallback function:\n",
        "\n",
        "<br/>\n",
        "\n",
        "| Parameter             | Description |\n",
        "| --------------------- | ----------- |\n",
        "| **task**              | Task for which the model is to be evaluated (text-classification or ner) |\n",
        "| **data**              | The data to be used for evaluation. A dictionary providing flexibility and options for data sources. It should include the following keys: <ul><li>data_source (mandatory): The source of the data.</li><li>subset (optional): The subset of the data.</li><li>feature_column (optional): The column containing the features.</li><li>target_column (optional): The column containing the target labels.</li><li>split (optional): The data split to be used.</li><li>source (optional): Set to 'huggingface' when loading Hugging Face dataset.</li></ul> |\n",
        "| **config**            | Configuration for the tests to be performed, specified in the form of a YAML file. |\n",
        "| **print_reports**     | A bool value that specifies if the reports should be printed. |\n",
        "| **save_reports**      | A bool value that specifies if the reports should be saved. |\n",
        "| **run_each_epoch**    | A bool value that specifies if the tests should be run after each epoch or the at the end of training |\n",
        "\n",
        "<br/>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_93RQeoDVsW"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers[torch]\n",
        "!pip install tensorflow -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/langtest/main/demo/data/conll03.conll\n",
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/langtest/main/langtest/data/conll/sample.conll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFzwOHkqC7tQ",
        "outputId": "d7dccbc0-1691-43a5-879a-0fc04e6b5a60"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertForTokenClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
        "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"conll03.conll\"\n",
        "\n",
        "def read_conll_file(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    return lines[::2]\n",
        "\n",
        "lines = read_conll_file(file_path)\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_conll(lines):\n",
        "    tokens = []\n",
        "    labels = []\n",
        "    token_list = []\n",
        "    label_list = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(\"-DOCSTART-\") or line == \"\\n\":\n",
        "            if token_list:\n",
        "                tokens.append(token_list)\n",
        "                labels.append(label_list)\n",
        "            token_list = []\n",
        "            label_list = []\n",
        "        else:\n",
        "            token, _, _, label = line.strip().split()\n",
        "            token_list.append(token)\n",
        "            label_list.append(label)\n",
        "\n",
        "    return tokens, labels\n",
        "\n",
        "tokens, labels = preprocess_conll(lines)\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, tokens, labels, tokenizer, max_length=128):\n",
        "        self.tokens = tokens\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.label_map = {label: i for i, label in enumerate(sorted(set([lbl for doc_labels in labels for lbl in doc_labels])))}\n",
        "        self.id2label = {v: k for k, v in self.label_map.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        token_list = self.tokens[idx]\n",
        "        label_list = self.labels[idx]\n",
        "\n",
        "        encoded = self.tokenizer(token_list, is_split_into_words=True, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
        "        token_ids = encoded.input_ids.squeeze(0)\n",
        "        attention_mask = encoded.attention_mask.squeeze(0)\n",
        "\n",
        "        label_ids = [self.label_map[label] for label in label_list]\n",
        "        label_ids = [-100] + label_ids + [-100]  # Account for [CLS] and [SEP] tokens\n",
        "        label_ids += [-100] * (self.max_length - len(label_ids))  # Pad labels\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": token_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": torch.tensor(label_ids, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer and dataset\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"dslim/bert-base-NER\")\n",
        "train_dataset = NERDataset(tokens, labels, tokenizer)\n",
        "\n",
        "# Initialize model\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"dslim/bert-base-NER\",\n",
        "    num_labels=len(train_dataset.label_map),\n",
        "    id2label=train_dataset.id2label,\n",
        "    label2id=train_dataset.label_map,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "# Initialize the classifier layer with the correct number of labels\n",
        "model.classifier = torch.nn.Linear(model.config.hidden_size, len(train_dataset.label_map))\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a LangTestCallback instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After loading the model and tokenizer from huggingface, we can get to the training part of our process. We will utilize `transformers.Trainer` for easily integrating our callback into the training process. We will also use `transformers.TrainingArguments` to specify the training arguments.\n",
        "\n",
        "We can store the config in a dictionary and pass it to the LangTestCallback function for easier use and visual appeal. The config will be used in this notebook is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZqT9vZQiC7tS"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        " \"tests\": {\n",
        "  \"defaults\": {\n",
        "   \"min_pass_rate\": 1.0\n",
        "  },\n",
        "  \"robustness\": {\n",
        "   \"add_typo\": {\"min_pass_rate\": 0.7},\n",
        "   \"uppercase\": {\"min_pass_rate\": 0.7},\n",
        "   \"american_to_british\": {\"min_pass_rate\": 0.7},\n",
        "  },\n",
        "  \"accuracy\": {\n",
        "   \"min_micro_f1_score\": {\n",
        "    \"min_score\": 0.7\n",
        "   }\n",
        "  },\n",
        "  \"bias\": {\n",
        "   \"replace_to_female_pronouns\": {\n",
        "    \"min_pass_rate\": 0.7\n",
        "   },\n",
        "   \"replace_to_low_income_country\": {\n",
        "    \"min_pass_rate\": 0.7\n",
        "   }\n",
        "  }\n",
        " }\n",
        "}\n",
        "my_callback = LangTestCallback(task=\"ner\", data={\"data_source\":\"sample.conll\"}, config=config, save_reports=True, run_each_epoch=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned earlier, we create a TrainingArguments object to specify the training arguments. We will also create a Trainer object to train our model. Then we can pass the LangTestCallback object to the Trainer object as a callback. LangTestCallback initilizes the harness object and generates the testcases using .generate() after the trainer is initialized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "325jnkfxfCPF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Configuration : \n",
            " {\n",
            " \"tests\": {\n",
            "  \"defaults\": {\n",
            "   \"min_pass_rate\": 1.0\n",
            "  },\n",
            "  \"robustness\": {\n",
            "   \"add_typo\": {\n",
            "    \"min_pass_rate\": 0.7\n",
            "   },\n",
            "   \"uppercase\": {\n",
            "    \"min_pass_rate\": 0.7\n",
            "   },\n",
            "   \"american_to_british\": {\n",
            "    \"min_pass_rate\": 0.7\n",
            "   }\n",
            "  },\n",
            "  \"accuracy\": {\n",
            "   \"min_micro_f1_score\": {\n",
            "    \"min_score\": 0.7\n",
            "   }\n",
            "  },\n",
            "  \"bias\": {\n",
            "   \"replace_to_female_pronouns\": {\n",
            "    \"min_pass_rate\": 0.7\n",
            "   },\n",
            "   \"replace_to_low_income_country\": {\n",
            "    \"min_pass_rate\": 0.7\n",
            "   }\n",
            "  }\n",
            " }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=64,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[my_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The actual training step is very simple. We just need to call the train() method of the Trainer object. We can also pass the training arguments to the train() method but its default values are OK in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have the reports printed and also saved under the reports folder. The reports are saved in the form of a MD file. The reports folder is created in the same directory as the notebook.We have the reports printed and also saved under the reports folder. The reports are saved in the form of a MD file. The reports folder is created in the same directory as the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PzAaW4CPC7tV",
        "outputId": "f11d85ee-36f2-4341-a761-1d305391790c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating testcases...: 100%|██████████| 3/3 [00:00<?, ?it/s]\n",
            "WARNING:root:[W009] Removing samples where no transformation has been applied:\n",
            "[W010] - Test 'add_typo': 14 samples removed out of 226\n",
            "[W010] - Test 'uppercase': 28 samples removed out of 226\n",
            "[W010] - Test 'american_to_british': 226 samples removed out of 226\n",
            "\n",
            "WARNING:root:[W009] Removing samples where no transformation has been applied:\n",
            "[W010] - Test 'replace_to_female_pronouns': 198 samples removed out of 226\n",
            "[W010] - Test 'replace_to_low_income_country': 138 samples removed out of 226\n",
            "\n",
            "Running testcases... : 100%|██████████| 527/527 [01:33<00:00,  5.62it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>test_type</th>\n",
              "      <th>fail_count</th>\n",
              "      <th>pass_count</th>\n",
              "      <th>pass_rate</th>\n",
              "      <th>minimum_pass_rate</th>\n",
              "      <th>pass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>robustness</td>\n",
              "      <td>add_typo</td>\n",
              "      <td>65</td>\n",
              "      <td>147</td>\n",
              "      <td>69%</td>\n",
              "      <td>70%</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>robustness</td>\n",
              "      <td>uppercase</td>\n",
              "      <td>129</td>\n",
              "      <td>69</td>\n",
              "      <td>35%</td>\n",
              "      <td>70%</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>accuracy</td>\n",
              "      <td>min_micro_f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>100%</td>\n",
              "      <td>100%</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bias</td>\n",
              "      <td>replace_to_female_pronouns</td>\n",
              "      <td>11</td>\n",
              "      <td>17</td>\n",
              "      <td>61%</td>\n",
              "      <td>70%</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bias</td>\n",
              "      <td>replace_to_low_income_country</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>50%</td>\n",
              "      <td>70%</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     category                      test_type  fail_count  pass_count  \\\n",
              "0  robustness                       add_typo          65         147   \n",
              "1  robustness                      uppercase         129          69   \n",
              "2    accuracy             min_micro_f1_score           0           1   \n",
              "3        bias     replace_to_female_pronouns          11          17   \n",
              "4        bias  replace_to_low_income_country          44          44   \n",
              "\n",
              "  pass_rate minimum_pass_rate   pass  \n",
              "0       69%               70%  False  \n",
              "1       35%               70%  False  \n",
              "2      100%              100%   True  \n",
              "3       61%               70%  False  \n",
              "4       50%               70%  False  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running testcases... : 100%|██████████| 527/527 [00:13<00:00, 39.39it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>test_type</th>\n",
              "      <th>fail_count</th>\n",
              "      <th>pass_count</th>\n",
              "      <th>pass_rate</th>\n",
              "      <th>minimum_pass_rate</th>\n",
              "      <th>pass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>robustness</td>\n",
              "      <td>add_typo</td>\n",
              "      <td>65</td>\n",
              "      <td>147</td>\n",
              "      <td>69%</td>\n",
              "      <td>70%</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>robustness</td>\n",
              "      <td>uppercase</td>\n",
              "      <td>129</td>\n",
              "      <td>69</td>\n",
              "      <td>35%</td>\n",
              "      <td>70%</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>accuracy</td>\n",
              "      <td>min_micro_f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>100%</td>\n",
              "      <td>100%</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bias</td>\n",
              "      <td>replace_to_female_pronouns</td>\n",
              "      <td>11</td>\n",
              "      <td>17</td>\n",
              "      <td>61%</td>\n",
              "      <td>70%</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bias</td>\n",
              "      <td>replace_to_low_income_country</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>50%</td>\n",
              "      <td>70%</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     category                      test_type  fail_count  pass_count  \\\n",
              "0  robustness                       add_typo          65         147   \n",
              "1  robustness                      uppercase         129          69   \n",
              "2    accuracy             min_micro_f1_score           0           1   \n",
              "3        bias     replace_to_female_pronouns          11          17   \n",
              "4        bias  replace_to_low_income_country          44          44   \n",
              "\n",
              "  pass_rate minimum_pass_rate   pass  \n",
              "0       69%               70%  False  \n",
              "1       35%               70%  False  \n",
              "2      100%              100%   True  \n",
              "3       61%               70%  False  \n",
              "4       50%               70%  False  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 2029.1679, 'train_samples_per_second': 1.67, 'train_steps_per_second': 0.027, 'train_loss': 0.7498808260317202, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=54, training_loss=0.7498808260317202, metrics={'train_runtime': 2029.1679, 'train_samples_per_second': 1.67, 'train_steps_per_second': 0.027, 'train_loss': 0.7498808260317202, 'epoch': 2.0})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training the model\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
